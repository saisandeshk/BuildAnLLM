"""Reusable Streamlit UI components."""

import streamlit as st
from typing import Dict


# Model size presets
MODEL_SIZE_PRESETS = {
    "small": {
        "d_model": 256,
        "n_heads": 4,
        "n_layers": 4,
        "n_ctx": 256,
        "d_head": 64,
        "d_mlp": 1024,
    },
    "medium": {
        "d_model": 512,
        "n_heads": 8,
        "n_layers": 6,
        "n_ctx": 512,
        "d_head": 64,
        "d_mlp": 2048,
    },
    "full": {
        "d_model": 768,
        "n_heads": 12,
        "n_layers": 12,
        "n_ctx": 1024,
        "d_head": 64,
        "d_mlp": 3072,
    },
}


def apply_model_size_preset(size: str, config: Dict) -> None:
    """Apply model size preset to config."""
    preset = MODEL_SIZE_PRESETS[size]
    for key, value in preset.items():
        config[key] = value


def apply_architecture_preset(preset_name: str, config: Dict) -> None:
    """Apply architecture preset (GPT, LLaMA, OLMo) to config."""
    if preset_name == "GPT":
        config["positional_encoding"] = "learned"
        config["normalization"] = "layernorm"
        config["activation"] = "gelu"
        config["tokenizer_type"] = "bpe"
    elif preset_name == "LLAMA":
        config["positional_encoding"] = "rope"
        config["normalization"] = "rmsnorm"
        config["activation"] = "swiglu"
        config["tokenizer_type"] = "sentencepiece"
        config["rope_theta"] = 10000.0
    elif preset_name == "OLMO":
        config["positional_encoding"] = "alibi"
        config["normalization"] = "layernorm"
        config["activation"] = "swiglu"
        config["tokenizer_type"] = "sentencepiece"


def render_model_config_ui() -> Dict:
    """Render model configuration UI and return config dict."""
    # Initialize config if needed
    if "model_config" not in st.session_state:
        st.session_state.model_config = _get_default_config()

    config = st.session_state.model_config

    # Preset buttons
    _render_preset_buttons(config)

    # Model components
    _render_model_components(config)

    # Model dimensions
    _render_model_dimensions(config)

    # Model size selector
    _render_model_size_selector(config)

    # RoPE settings (conditional)
    if config["positional_encoding"] == "rope":
        _render_rope_settings(config)

    return config


def _get_default_config() -> Dict:
    """Get default model configuration."""
    return {
        "positional_encoding": "learned",
        "normalization": "layernorm",
        "activation": "gelu",
        "model_size": "small",
        "d_model": 256,
        "n_heads": 4,
        "n_layers": 4,
        "n_ctx": 256,
        "d_head": 64,
        "d_mlp": 1024,
        "rope_theta": 10000.0,
        "tokenizer_type": "bpe",
    }


def _render_preset_buttons(config: Dict) -> None:
    """Render architecture preset buttons."""
    st.subheader("Quick Presets")
    col1, col2, col3, col4 = st.columns([1, 1, 1, 2])

    with col1:
        if st.button("ğŸš€ GPT-2", use_container_width=True):
            apply_architecture_preset("GPT", config)
            apply_model_size_preset(config.get("model_size", "small"), config)
            st.rerun()

    with col2:
        if st.button("ğŸ¦™ LLaMA", use_container_width=True):
            apply_architecture_preset("LLAMA", config)
            apply_model_size_preset(config.get("model_size", "small"), config)
            st.rerun()

    with col3:
        if st.button("ğŸ”¬ OLMo", use_container_width=True):
            apply_architecture_preset("OLMO", config)
            apply_model_size_preset(config.get("model_size", "small"), config)
            st.rerun()

    with col4:
        with st.expander("â„¹ï¸ About Presets", expanded=False):
            st.markdown(_get_preset_info())


def _render_model_components(config: Dict) -> None:
    """Render model component selectors."""
    st.subheader("Model Components")
    col1, col2, col3 = st.columns(3)

    with col1:
        config["positional_encoding"] = st.selectbox(
            "Positional Encoding",
            ["learned", "rope", "alibi", "none"],
            index=["learned", "rope", "alibi", "none"].index(
                config["positional_encoding"]
            ),
            help="Learned: GPT-style embeddings\nRoPE: Rotary Position Embedding (LLaMA)\nALiBi: Attention with Linear Biases (OLMo)\nNone: No positional encoding"
        )

    with col2:
        config["normalization"] = st.selectbox(
            "Normalization",
            ["layernorm", "rmsnorm"],
            index=["layernorm", "rmsnorm"].index(config["normalization"]),
            help="LayerNorm: GPT/OLMo style\nRMSNorm: LLaMA style (simpler, faster)"
        )

    with col3:
        config["activation"] = st.selectbox(
            "Activation Function",
            ["gelu", "swiglu"],
            index=["gelu", "swiglu"].index(config["activation"]),
            help="GELU: GPT style\nSwiGLU: LLaMA/OLMo style (gated)"
        )


def _render_model_dimensions(config: Dict) -> None:
    """Render model dimension inputs."""
    st.subheader("Model Dimensions")
    col1, col2, col3 = st.columns(3)

    with col1:
        config["d_model"] = st.number_input(
            "d_model (Model Dimension)",
            min_value=64, max_value=4096, value=config["d_model"], step=64,
            help="Hidden dimension size"
        )
        config["n_heads"] = st.number_input(
            "n_heads (Number of Heads)",
            min_value=1, max_value=64, value=config["n_heads"],
            help="Number of attention heads"
        )

    with col2:
        config["n_layers"] = st.number_input(
            "n_layers (Number of Layers)",
            min_value=1, max_value=128, value=config["n_layers"],
            help="Number of transformer layers"
        )
        config["n_ctx"] = st.number_input(
            "n_ctx (Context Length)",
            min_value=64, max_value=8192, value=config["n_ctx"], step=64,
            help="Maximum sequence length"
        )

    with col3:
        config["d_head"] = st.number_input(
            "d_head (Head Dimension)",
            min_value=32, max_value=256, value=config["d_head"], step=32,
            help="Dimension per attention head"
        )
        config["d_mlp"] = st.number_input(
            "d_mlp (MLP Dimension)",
            min_value=128, max_value=16384, value=config["d_mlp"], step=128,
            help="MLP hidden dimension (typically 4x d_model)"
        )


def _render_model_size_selector(config: Dict) -> None:
    """Render model size selector."""
    st.markdown("**Model Size Preset**")
    model_size = st.selectbox(
        "Size",
        ["small", "medium", "full"],
        index=["small", "medium", "full"].index(config.get("model_size", "small")),
        help="Selecting a size automatically updates all model dimensions below."
    )

    if config.get("model_size") != model_size:
        config["model_size"] = model_size
        apply_model_size_preset(model_size, config)
        st.rerun()

    config["model_size"] = model_size


def _render_rope_settings(config: Dict) -> None:
    """Render RoPE-specific settings."""
    config["rope_theta"] = st.number_input(
        "RoPE Theta (Base Frequency)",
        min_value=1000.0, max_value=1000000.0,
        value=config["rope_theta"], step=1000.0, format="%.0f",
        help="Base frequency for RoPE. LLaMA 1/2: 10000, LLaMA 3: 500000"
    )


def _get_preset_info() -> str:
    """Get preset information markdown."""
    return """
    **Preset Configurations:**
    - **GPT-2**: Learned positional embeddings, LayerNorm, GELU activation, BPE tokenizer
    - **LLaMA**: RoPE positional encoding, RMSNorm, SwiGLU activation, SentencePiece tokenizer
    - **OLMo**: ALiBi positional encoding, LayerNorm, SwiGLU activation, SentencePiece tokenizer

    **Model Size:**
    - Controls model dimensions (d_model, n_heads, n_layers, etc.)
    - All presets use the same dimensions for each size
    - Clicking a preset uses the currently selected model size

    **Customization:**
    - All options below can be manually adjusted after selecting a preset
    - Tokenizer is automatically set but can be changed
    """


def generate_model_architecture_diagram(config: Dict) -> str:
    """Generate ASCII art diagram of transformer architecture."""
    n_layers = config.get("n_layers", 4)
    d_model = config.get("d_model", 256)
    n_heads = config.get("n_heads", 4)
    d_mlp = config.get("d_mlp", 1024)
    pos_enc = config.get("positional_encoding", "learned")
    norm = config.get("normalization", "layernorm")
    activation = config.get("activation", "gelu")

    # Map technical names to display names
    pos_enc_display = {
        "learned": "Learned Pos Emb",
        "rope": "RoPE",
        "alibi": "ALiBi",
        "none": "None"
    }.get(pos_enc, pos_enc)

    norm_display = {
        "layernorm": "LayerNorm",
        "rmsnorm": "RMSNorm"
    }.get(norm, norm)

    activation_display = {
        "gelu": "GELU",
        "swiglu": "SwiGLU"
    }.get(activation, activation)

    # Build the diagram
    diagram = []

    # Title
    diagram.append(f"Transformer Architecture ({n_layers} layers, d_model={d_model})")
    diagram.append("="*60)
    diagram.append("")

    # Input section
    diagram.append("                         INPUT")
    diagram.append("                           |")
    diagram.append("                           v")
    diagram.append("                   [Token Embeddings]")
    diagram.append(f"                    (vocab â†’ {d_model})")
    diagram.append("                           |")
    if pos_enc == "learned":
        diagram.append("                           +")
        diagram.append(f"                   [{pos_enc_display}]")
        diagram.append("                           |")
    diagram.append("                           v")
    diagram.append("          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    diagram.append("          â”‚                                â”‚")
    diagram.append("          â”‚      RESIDUAL STREAM           â”‚")
    diagram.append(f"          â”‚         (d={d_model:4})               â”‚")
    diagram.append("          â”‚                                â”‚")

    # Layers
    for layer_idx in range(n_layers):
        diagram.append(f"          â”‚  â”€ â”€ â”€ Layer {layer_idx + 1} â”€ â”€ â”€ â”€ â”€ â”€     â”‚")
        diagram.append("          â”‚                                â”‚")

        # Attention block
        diagram.append(f"          â”œâ”€â”€â”€â”€â”€â”€> [{norm_display}] â”€â”€â”€â”€â”€â”      â”‚")
        diagram.append("          â”‚                         â”‚      â”‚")
        diagram.append("          â”‚       [Multi-Head       â”‚      â”‚")
        diagram.append("          â”‚        Attention]       â”‚      â”‚")
        diagram.append(f"          â”‚       ({n_heads} heads)         â”‚      â”‚")

        # Add positional encoding info inside attention block
        if pos_enc == "rope":
            diagram.append("          â”‚    (RoPE on Q,K)       â”‚      â”‚")
        elif pos_enc == "alibi":
            diagram.append("          â”‚   (ALiBi bias added)   â”‚      â”‚")
        else:
            diagram.append("          â”‚                         â”‚      â”‚")

        diagram.append("          â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚")
        diagram.append("          â”‚            +                   â”‚")
        diagram.append("          â”‚                                â”‚")

        # MLP block
        diagram.append(f"          â”œâ”€â”€â”€â”€â”€â”€> [{norm_display}] â”€â”€â”€â”€â”€â”      â”‚")
        diagram.append("          â”‚                         â”‚      â”‚")
        diagram.append("          â”‚         [MLP]           â”‚      â”‚")
        diagram.append(f"          â”‚    ({d_model}â†’{d_mlp}â†’{d_model})       â”‚      â”‚")
        diagram.append(f"          â”‚        [{activation_display}]           â”‚      â”‚")
        diagram.append("          â”‚                         â”‚      â”‚")
        diagram.append("          â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚")
        diagram.append("          â”‚            +                   â”‚")
        diagram.append("          â”‚                                â”‚")

    # Output section
    diagram.append("          â”‚  â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€      â”‚")
    diagram.append("          â”‚                                â”‚")
    diagram.append(f"          â”‚        [{norm_display}]            â”‚")
    diagram.append("          â”‚                                â”‚")
    diagram.append("          â”‚        [Unembedding]           â”‚")
    diagram.append(f"          â”‚      ({d_model} â†’ vocab)            â”‚")
    diagram.append("          â”‚                                â”‚")
    diagram.append("          â”‚              v                 â”‚")
    diagram.append("          â”‚           OUTPUT               â”‚")
    diagram.append("          â”‚          (logits)              â”‚")
    diagram.append("          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

    return "\n".join(diagram)


def generate_graphviz_architecture(config: Dict) -> str:
    """Generate Graphviz DOT code for transformer architecture."""
    n_layers = config.get("n_layers", 4)
    n_heads = config.get("n_heads", 4)
    pos_enc = config.get("positional_encoding", "learned")
    activation = config.get("activation", "gelu")

    # Start building the DOT code
    dot = []
    dot.append('digraph TransformerArchitecture {')
    dot.append('    bgcolor="black";')
    dot.append('    rankdir=BT;')  # Bottom to top like the reference
    dot.append('    splines=ortho;')
    dot.append('    nodesep=0.6;')
    dot.append('    ranksep=0.7;')

    # Node styles
    dot.append('    node [shape=box, style=filled, fillcolor="#5a5a5a", fontcolor="white", ')
    dot.append('          fontname="Arial", fontsize=10, height=0.5, width=1.2];')
    dot.append('    edge [color="#aaaaaa", penwidth=1.5, arrowsize=0.7];')
    dot.append('')

    # Create nodes
    dot.append('    // Input/Output nodes')
    dot.append('    tokens [label="tokens", fillcolor="#4a4a4a"];')
    dot.append('    embed [label="embed", fillcolor="#6a6a4a"];')
    dot.append('    logits [label="logits", fillcolor="#4a4a4a"];')
    dot.append('    unembed [label="unembed", fillcolor="#6a6a4a"];')

    # Create x nodes (residual stream points)
    dot.append('')
    dot.append('    // Residual stream points')
    dot.append('    x0 [shape=plaintext, label="xâ‚€", fontcolor="#cccccc"];')

    for i in range(n_layers):
        dot.append(f'    x{i+1} [shape=plaintext, label="x_{i+1}", fontcolor="#cccccc"];')
        dot.append(f'    x{i+1}_post [shape=plaintext, label="x_{i+2}", fontcolor="#cccccc"];')

    dot.append(f'    x_final [shape=plaintext, label="x_{n_layers-1}", fontcolor="#cccccc"];')

    # Create attention and MLP blocks
    dot.append('')
    dot.append('    // Attention and MLP blocks')
    for i in range(n_layers):
        layer = i + 1

        # Attention heads
        heads_label = f"hâ‚€  hâ‚  ...  h_{n_heads-1}"
        if pos_enc == "rope":
            heads_label += "\\n(RoPE)"
        elif pos_enc == "alibi":
            heads_label += "\\n(ALiBi)"

        dot.append(f'    heads{layer} [label="{heads_label}", fillcolor="#6a5a5a"];')

        # MLP
        mlp_label = "MLP  m"
        if activation == "swiglu":
            mlp_label += "\\n(SwiGLU)"
        elif activation == "gelu":
            mlp_label += "\\n(GELU)"

        dot.append(f'    mlp{layer} [label="{mlp_label}", fillcolor="#5a6a5a"];')

    dot.append('')
    dot.append('    // Connections')

    # Input flow
    dot.append('    tokens -> embed;')

    # Handle positional encoding
    if pos_enc == "learned":
        dot.append('    embed -> x0 [label="+PE", fontsize=8, fontcolor="yellow"];')
    else:
        dot.append('    embed -> x0;')

    # Layer connections
    for i in range(n_layers):
        layer = i + 1
        prev_x = 'x0' if i == 0 else f'x{i}_post'
        curr_x = f'x{layer}'
        post_x = f'x{layer}_post'

        # Main residual stream
        dot.append(f'    {prev_x} -> {curr_x};')

        # Attention branch
        dot.append(f'    {curr_x} -> heads{layer};')
        dot.append(f'    heads{layer} -> {curr_x} [label="+", fontsize=10, fontcolor="yellow"];')

        # Continue to MLP
        dot.append(f'    {curr_x} -> {post_x};')

        # MLP branch
        dot.append(f'    {post_x} -> mlp{layer};')
        dot.append(f'    mlp{layer} -> {post_x} [label="+", fontsize=10, fontcolor="yellow"];')

    # Output
    last_x = f'x{n_layers}_post'
    dot.append(f'    {last_x} -> x_final;')
    dot.append('    x_final -> unembed;')
    dot.append('    unembed -> logits;')

    dot.append('}')

    return '\n'.join(dot)


def render_model_architecture_diagram(config: Dict) -> None:
    """Render the model architecture diagram in Streamlit."""
    with st.expander("ğŸ—ï¸ Model Architecture Diagram", expanded=False):
        # Add tabs for different diagram types
        tab1, tab2 = st.tabs(["ASCII Diagram", "Graphviz Diagram"])

        with tab1:
            diagram = generate_model_architecture_diagram(config)
            st.code(diagram, language="text")

            # Add explanation
            st.markdown("""
            **Diagram Legend:**
            - The **Residual Stream** (right side) carries information through the network
            - Components branch off to process information and add it back
            - Each layer has two main blocks: **Attention** and **MLP**
            - Both blocks use normalization and have residual connections (+)
            - The stream preserves dimension d_model throughout the network
            """)

        with tab2:
            try:
                import graphviz
                dot_code = generate_graphviz_architecture(config)
                st.graphviz_chart(dot_code)
            except ImportError:
                st.warning("Graphviz is not installed. Install it with: `pip install graphviz`")
                st.code(generate_graphviz_architecture(config), language="dot")